{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                  House Prices : Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aim: Predict the sale price of a house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features (80) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MSSubClass,\n",
    "MSZoning,\n",
    "LotFrontage,\n",
    "LotArea,\n",
    "Street,\n",
    "Alley,\n",
    "LotShape,\n",
    "LandContour,\n",
    "Utilities,\n",
    "LotConfig,\n",
    "LandSlope,\n",
    "Neighborhood,\n",
    "Condition1,\n",
    "Condition2,\n",
    "BldgType,\n",
    "HouseStyle,\n",
    "OverallQual,\n",
    "OverallCond,\n",
    "YearBuilt,\n",
    "YearRemodAdd,\n",
    "RoofStyle,\n",
    "RoofMatl,\n",
    "Exterior1st,\n",
    "Exterior2nd,\n",
    "MasVnrType,\n",
    "MasVnrArea,\n",
    "ExterQual,\n",
    "ExterCond,\n",
    "Foundation,\n",
    "BsmtQual,\n",
    "BsmtCond,\n",
    "BsmtExposure,\n",
    "BsmtFinType1,\n",
    "BsmtFinSF1,\n",
    "BsmtFinType2,\n",
    "BsmtFinSF2,\n",
    "BsmtUnfSF,\n",
    "TotalBsmtSF,\n",
    "Heating,\n",
    "HeatingQC,\n",
    "CentralAir,\n",
    "Electrical,\n",
    "1stFlrSF,\n",
    "2ndFlrSF,\n",
    "LowQualFinSF,\n",
    "GrLivArea,\n",
    "BsmtFullBath,\n",
    "BsmtHalfBath,\n",
    "FullBath,\n",
    "HalfBath,\n",
    "Bedroom,\n",
    "Kitchen,\n",
    "KitchenQual,\n",
    "TotRmsAbvGrd,\n",
    "Functional,\n",
    "Fireplaces,\n",
    "FireplaceQu,\n",
    "GarageType,\n",
    "GarageYrBlt,\n",
    "GarageFinish,\n",
    "GarageCars,\n",
    "GarageArea,\n",
    "GarageQual,\n",
    "GarageCond,\n",
    "PavedDrive,\n",
    "WoodDeckSF,\n",
    "OpenPorchSF,\n",
    "EnclosedPorch,\n",
    "3SsnPorch,\n",
    "ScreenPorch,\n",
    "PoolArea,\n",
    "PoolQC,\n",
    "Fence,\n",
    "MiscFeature,\n",
    "MiscVal,\n",
    "MoSold,\n",
    "YrSold,\n",
    "SaleType,\n",
    "SaleCondition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import sys \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**------------------------------------------------------------ 1. LOADING & LOOKING AT THE DATA --------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The housing dataset is available on Kaggle under “House Prices: Advanced Regression Techniques”. The “train.csv” file contains the training data and “test.csv” contains the testing data. The training data contains data for 1460 rows which corresponds to 1460 house’s data and   80 columns which correspond to the feature of those houses. Similarly, the testing data contains data of 1461 houses and their 79 attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset \n",
    "csv_path = \"train.csv\"\n",
    "df_train = pd.read_csv(csv_path, sep = ',')  \n",
    "\n",
    "csv_path = \"test.csv\"\n",
    "df_test = pd.read_csv(csv_path, sep = ',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look a first 10 rows of training data\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look a first 10 rows of testing data\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the column names\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 1460 rows and 81 columns\n",
    "- There are columns with large number of null entries like PoolQC, MiscFeature\n",
    "- The columns have Three types of datatypes: float64(3), int64(35), object(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 1459 rows and 80 columns\n",
    "- There are columns with large number of null entries like PoolQC, MiscFeature etc\n",
    "- The columns have Three types of datatypes:  float64(11), int64(26), object(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the label to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average SalePrice of a house is 180,921\n",
    "- The Maximum SalePrice of a house is 755,000 and Minimum 34,900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corr_mat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "sns.heatmap(corr_mat, vmax=.8,square=True)\n",
    "\n",
    "plt.suptitle(\"Correlatation Feature HeatMap\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most correlated features\n",
    "corr_mat = df_train.corr()\n",
    "\n",
    "sns.set(font_scale = 1.3)\n",
    "plt.figure(figsize = (11,8))\n",
    "\n",
    "top_corr = corr_mat.index[abs(corr_mat[\"SalePrice\"])>0.5]\n",
    "g = sns.heatmap(df_train[top_corr].corr(),annot=True,cmap=\"YlGnBu\")\n",
    "plt.suptitle(\"Top Correlated Feature HeatMap (Correlation > 0.5 with Sale Price)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OverallQual and GrLivArea seem to be the most correlated to SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Values\")\n",
    "\n",
    "corr = df_train.corr().drop('SalePrice')\n",
    "corr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\n",
    "print(corr.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars','GarageArea', 'TotalBsmtSF','1stFlrSF','FullBath','YearBuilt']\n",
    "sns_plot = sns.pairplot(df_train[cols])\n",
    "\n",
    "plt.suptitle('Scatter plots between top 9 most corr features', y=1.04, size=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 5,5\n",
    "cols = ['SalePrice','EnclosedPorch', 'KitchenAbvGr', 'MSSubClass', 'LowQualFinSF','YrSold', 'OverallCond']\n",
    "sns_plot = sns.pairplot(df_train[cols])\n",
    "\n",
    "plt.suptitle('Scatter plots between least 6  corr features', y=1.04, size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------- 2. HANDLING DATA --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Id Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop id as it is not required for training or prediction\n",
    "train_ID = df_train['Id']\n",
    "test_ID = df_test['Id']\n",
    "\n",
    "df_train.drop(['Id'], axis=1, inplace=True)\n",
    "df_test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "edgecolor = 'black'\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "#function to plot scatter plot between a feature and the Sale Price \n",
    "def scatter_plot(a):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x = df_train[a], y = df_train['SalePrice'], edgecolor=edgecolor)\n",
    "    plt.ylabel('SalePrice', fontsize=12)\n",
    "    plt.xlabel(a, fontsize=12)\n",
    "    plt.suptitle(\"Scatter Plot of \"+ a + \" and SalePrice\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot('GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- It can be observed that there are large outliers which can negatively affect the prediction of sale price highly\n",
    "- So the outliers need to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "df_train =  df_train.drop( df_train[( df_train['GrLivArea'] > 4000) & ( df_train['SalePrice']<300000)].index)\n",
    "\n",
    "#Check the graphic again\n",
    "scatter_plot('GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There arent too large outliers, we do not need to delete any points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot('EnclosedPorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is are some outliers that should be deleted so that it doesnt affect our predictions much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "df_train =  df_train.drop( df_train[( df_train['EnclosedPorch']>400)].index)\n",
    "\n",
    "#Deleting outliers\n",
    "df_train =  df_train.drop( df_train[( df_train['SalePrice']>700000)].index)\n",
    "\n",
    "#check plot again\n",
    "scatter_plot('EnclosedPorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a box plot for categorical feature : Overall Quality\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\n",
    "sns.boxplot(x = df_train['OverallQual'], y=\"SalePrice\", data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a box plot for categorical feature : Year Built\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "\n",
    "data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\n",
    "sns.boxplot(x= df_train['YearBuilt'], y=\"SalePrice\", data=data)\n",
    "plt.xticks(rotation=90,fontsize= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_train['SalePrice'])\n",
    "\n",
    "plt.suptitle( \"Plot of Sale Price\")\n",
    "\n",
    "print(\"Skewness: %f\" % df_train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying log transformation to correct the positive skewness in the data\n",
    "# taking logs means that errors in predicting expensive and cheap houses will affect the result equally\n",
    "\n",
    "df_train['SalePrice'] = np.log(df_train['SalePrice'])\n",
    "plt.suptitle(\"Plot of Sale Price after log transformation\")\n",
    "sns.distplot(df_train['SalePrice'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to see the missing data in a dataframe\n",
    "def missing_data(df,n):    \n",
    "    total = df.isnull().sum().sort_values(ascending=False)          # Total No of missing values\n",
    "    percentage = (df.isnull().sum() / df.isnull().count()).sort_values(ascending=False)*100  # % of Missing values\n",
    "    No_unique_val = df.nunique()                                   # No of unique values\n",
    "    missing_data = pd.concat([total, percentage, No_unique_val], axis=1, \n",
    "                             keys=['Total No of missing val', '% of Missing val','No of unique val'], sort = False)\n",
    "    \n",
    "    print(missing_data.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data    \n",
    "missing_data(df_train,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PoolQC'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PoolQC,Alley have only two unique values\n",
    "- PoolQC has 99.7% of missing data, which means most of the values are NA: No Pool ie most of the houses do not have a pool\n",
    "- PoolQC,Alley,MiscFeature will be dropped due to large number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data \n",
    "missing_data(df_test,34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Utilities'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all records mostly \"AllPub\" for Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PoolQC,Alley,MiscFeature will be dropped due to large number of missing values\n",
    "- Utilities has only 1 unique value\n",
    "- Utility will also be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of null values in training data\n",
    "null_train = df_train.isnull().sum().sum()\n",
    "print(null_train)\n",
    "\n",
    "# calculate total number of null values in test data\n",
    "null_test = df_test.isnull().sum().sum()\n",
    "print(null_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the 'SalePrice'column as train_label\n",
    "train_label = df_train['SalePrice'].reset_index(drop=True)\n",
    "\n",
    "# # drop 'SalePrice' column from df_train \n",
    "df_train = df_train.drop(['SalePrice'], axis=1)\n",
    "# # now df_train contains all training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to HANDLE the missing data in a dataframe\n",
    "def missing (df):\n",
    "    \n",
    "    # drop theses columns due to large null values or many same values\n",
    "    df = df.drop(['Utilities','PoolQC','MiscFeature','Alley'], axis=1)\n",
    "    \n",
    "    # Null value likely means No Fence so fill as \"None\"\n",
    "    df[\"Fence\"] = df[\"Fence\"].fillna(\"None\") \n",
    "    \n",
    "    # Null value likely means No Fireplace so fill as \"None\"\n",
    "    df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")\n",
    "    \n",
    "    # Lot frontage is the feet of street connected to property, which is likely similar to the neighbourhood houses, so fill Median value\n",
    "    df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].median())\n",
    "    \n",
    "    # Null value likely means  typical(Typ)\n",
    "    df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")\n",
    "    \n",
    "    # Only one null value so fill as the most frequent value(mode)\n",
    "    df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])  \n",
    "    \n",
    "    # Only one null value so fill as the most frequent value(mode)\n",
    "    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n",
    "    \n",
    "    # Very few null value so fill with the most frequent value(mode)\n",
    "    df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n",
    "    \n",
    "    # Null value likely means no masonry veneer\n",
    "    df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\") #so fill as \"None\" (since categorical feature)\n",
    "    df[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)      #so fill as o\n",
    "    \n",
    "    # Only one null value so fill as the most frequent value(mode)\n",
    "    df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n",
    "    df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n",
    "    \n",
    "    #MSZoning is general zoning classification,Very few null value so fill with the most frequent value(mode)\n",
    "    df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\n",
    "    \n",
    "    #Null value likely means no Identified type of dwelling so fill as \"None\"\n",
    "    df['MSSubClass'] = df['MSSubClass'].fillna(\"None\")\n",
    "    \n",
    "    # Null value likely means No Garage, so fill as \"None\" (since these are categorical features)\n",
    "    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "        df[col] = df[col].fillna('None')\n",
    "    \n",
    "    # Null value likely means No Garage and no cars in garage, so fill as 0\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Null value likely means No Basement, so fill as 0\n",
    "    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Null value likely means No Basement, so fill as \"None\" (since these are categorical features)\n",
    "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "        df[col] = df[col].fillna('None')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = missing(df_train)\n",
    "df_test = missing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of null values in training data\n",
    "null_train = df_train.isnull().sum().sum()\n",
    "print(null_train)\n",
    "\n",
    "# calculate total number of null values in test data\n",
    "null_test = df_test.isnull().sum().sum()\n",
    "print(null_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_cols(df):\n",
    "    \n",
    "    df['Total_SF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] \n",
    "                             + (0.5 * df['BsmtHalfBath']))\n",
    "\n",
    "    df['Total_Porch_SF'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + \n",
    "                            df['ScreenPorch'] + df['WoodDeckSF'])\n",
    "\n",
    "    df['Total_Square_Feet'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n",
    "    \n",
    "    df['Total_Quality'] = df['OverallQual'] + df['OverallCond']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the new columns\n",
    "df_train = add_new_cols(df_train)\n",
    "df_test = add_new_cols(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "g1 = df_train.columns.to_series().groupby(df_train.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k.name: v for k, v in g1.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "g2 = df_test.columns.to_series().groupby(df_test.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k.name: v for k, v in g2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dummy values for categorical data\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#align the training and testing data\n",
    "df_train, df_test = df_train.align(df_test, join = 'inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total number of null values in training data\n",
    "null_train = df_train.isnull().sum().sum()\n",
    "print(null_train)\n",
    "\n",
    "# calculate total number of null values in test data\n",
    "null_test = df_test.isnull().sum().sum()\n",
    "print(null_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test           # testing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"SalePrice\"] = train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set = train_test_split(df_train,train_size= 0.7, shuffle=False)\n",
    "\n",
    "X_train = train_set.drop([\"SalePrice\"], axis=1)  # training features\n",
    "y_train = train_set[\"SalePrice\"].copy()             # training label\n",
    "\n",
    "X_valid = valid_set.drop([\"SalePrice\"], axis=1)  # testing features\n",
    "y_valid = valid_set[\"SalePrice\"].copy()               # testing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print()\n",
    "print(\"X_valid shape: {}\".format(X_valid.shape))\n",
    "print(\"y_valid shape: {}\".format(y_valid.shape))\n",
    "print()\n",
    "print(\"X_test shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data type and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_t_x = X_train.isnull().sum().sum()\n",
    "print(null_t_x)\n",
    "\n",
    "null_t_y = y_train.isnull().sum().sum()\n",
    "print(null_t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_v_x = X_valid.isnull().sum().sum()\n",
    "print(null_v_x)\n",
    "\n",
    "null_v_y = y_valid.isnull().sum().sum()\n",
    "print(null_v_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No null values in X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 5 null values in y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values by mean value of y_valid column\n",
    "mean = np.nanmean(y_valid)\n",
    "y_valid = np.nan_to_num(y_valid,nan = mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check again\n",
    "np.where(np.isnan(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valid data shape:\")\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------- 3. SET CROSS VALIDATION AND RMSE --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- done to avoid underfitting/overfitting of data and to get a better understanging of how good our models are performing\n",
    "- split  data into k subsets, and train on k-1 of those subset,leaving one for testing\n",
    "- performing 10-fold cross validation for each model#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cross validation score with scoring set to negative mean absolute error\n",
    "def cross_validation(model):\n",
    "    \n",
    "    scores = np.sqrt(-cross_val_score(model, X_train, y_train, cv = 12, scoring = \"neg_mean_squared_error\"))\n",
    "    mean = np.mean(scores)\n",
    "    print(\"Mean CV score: \",mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate Root mean square error (RMSE)\n",
    "def rmse(y_pred, y_train): \n",
    "    \n",
    "    rmse_ = np.sqrt(metrics.mean_squared_error(y_pred,y_train))\n",
    "    print(\"rmse: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot actual vs predicited label\n",
    "def actual_vs_pred_plot(y_train,y_pred):\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.scatter(y_train, y_pred,color = \"teal\",edgecolor = 'lightblue')\n",
    "    ax.plot([y_train.min(),y_train.max()], [y_train.min(), y_train.max()], 'k--',lw=0.2)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.suptitle(\"Actual vs Predicted Scatter Plot\",size=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------- 4. DATA MODELLING  -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LINEAR REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression is the first model used. In this model, the target value is expected to be a linear combination of the features. The coefficients are set to minimize the residual sum of squares between the target predicted and the observed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit on training\n",
    "model_reg = reg.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y1_pred = reg.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y1_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y1_pred_v = reg.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y1_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "actual_vs_pred_plot(y_valid,y1_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. RIDGE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second model used is Ridge Regression. Ridge Regression is a regularized version of linear regression. The parameter alpha is used to regularize the model. For alpha equal to zero, ridge regression is just a linear regression. RidgeCV model is used to implement ridge regression as it has a built-in cross validation of the alpha parameter. Sixteen different values of alpha between 7e-4 and 20 were used with a 10-fold cross validation. A pipeline using min-max scaler was built to apply to training, validation and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the best value of alphas from this list, i will use RidgeCV\n",
    "alphas_ = [ 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20]\n",
    "\n",
    "# use robust scaler as unlike other scalers, the centering and scaling of ro bust scaler\n",
    "#is based on percentiles and are therefore is not influenced by a few number of very large marginal outliers.\n",
    "\n",
    "ridge = make_pipeline(MinMaxScaler(), linear_model.RidgeCV(alphas = alphas_, cv = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_ridge = ridge.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y2_pred = ridge.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y2_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the valid set\n",
    "y2_pred_v = ridge.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y2_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "actual_vs_pred_plot(y_train,y2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LASSO MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso regression is also a regularized version of linear regression. Lasso regression automatically performs feature selection and can estimates sparse coefficients.  LassoCV model was used to implement lasso regression as it has a built-in cross validation of the alpha parameter. Different values of alpha were set with a 10-fold cross validation. Robust scaler was used in a pipeline to scale the training, validation and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the best value of alphas from this list, i will use LassoCV\n",
    "alpha2 = [0.0001, 0.0002, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "\n",
    "#use robust scaler so that predictions are not influenced by a few number of very large marginal outliers\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), linear_model.LassoCV(alphas = alpha2, random_state=42,cv=12,max_iter=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_lasso = lasso.fit(X_train, y_train)\n",
    "\n",
    "#predict value of quality on the training set\n",
    "y3_pred = lasso.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y3_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y3_pred_v = lasso.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y3_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_pred_plot(y_valid,y3_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. K-NEAREST NEIGHBOUR REGRESSION MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K -nearest neighbour regressor is another popular model for regression tasks. It is a simple supervised machine learning model. The numbers of neighbours were set to three different values and the performance of this model was noted. Weights were set to uniform to assign equal weights to all points in each neighbourhood. The algorithm used was set to auto so that the best performing algorithm on the values was used. The leaf size was set to 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# N = 5 #\n",
    "neigh = KNeighborsRegressor(n_neighbors = 5,\n",
    "                            weights = 'uniform',\n",
    "                            algorithm = 'auto',\n",
    "                            leaf_size=25)\n",
    "neigh.fit(X_train,y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y4_pred = neigh.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y4_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 7 #\n",
    "neigh1 = KNeighborsRegressor(n_neighbors = 7,\n",
    "                             weights = 'uniform',\n",
    "                             leaf_size=25)\n",
    "neigh1.fit(X_train,y_train)\n",
    "\n",
    "#predict value of quality on the training set\n",
    "y_pred = neigh1.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 9 #\n",
    "neigh2 = KNeighborsRegressor(n_neighbors = 9,\n",
    "                             weights = 'uniform',\n",
    "                             leaf_size=25)\n",
    "neigh2.fit(X_train,y_train)\n",
    "\n",
    "#predict value of quality on the training set\n",
    "y_pred = neigh2.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=5 performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y4_pred_v = neigh.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y4_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: rmse increases when values of k(no. of neighbours) increase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_pred_plot(y_valid,y4_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. DECISION TREE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision tree model is also used to fit this data as it does not require much data cleaning and is not influenced by outliers. Decision trees can, unlike linear models, fit linearly inseparable datasets. The values of minimum leaves were set between 1 to 9 because a very small number of minimum leaves can cause overfitting whereas a large number of minimum leaves will prevent the tree from learning. Maximum depth of 7 and 9 were used to fit the data for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max depth to 5\n",
    "tree_regr1 = tree.DecisionTreeRegressor(max_depth = 7, min_samples_leaf=5,random_state=42)\n",
    "\n",
    "# set max depth to 9\n",
    "tree_regr2 = tree.DecisionTreeRegressor(max_depth = 9,min_samples_leaf=9,random_state=42)\n",
    "\n",
    "#fit the traning data to a decision tree model\n",
    "tree_regr11 = tree_regr1.fit(X_train,y_train)\n",
    "tree_regr12 = tree_regr2.fit(X_train,y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y1 = tree_regr1.predict(X_train)\n",
    "y2 = tree_regr2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(tree_regr1)\n",
    "cross_validation(tree_regr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate root mean square error\n",
    "rmse(y1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y5_pred_v = tree_regr2.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y5_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "actual_vs_pred_plot(y_valid,y5_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Random Forest MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest model is an ensemble method based on randomized decision trees. Grid search was used to select the best parameters with a 5-fold cross validation. The number of trees in the forest was set to 200 with a maximum depth of 5 and 3 minimum leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestRegressor(n_estimators=200,max_depth=13,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search to find best value of C, gamma and epsilon\n",
    "param_grid  = {'n_estimators': [100,150,200,250,300,350,400],\n",
    "               'max_depth': [5,7,9,11,13,15,17], \n",
    "               'min_samples_leaf': [3,5,7,9,11,13,15]}\n",
    "\n",
    "# set cross validation to 5\n",
    "clf = GridSearchCV(rforest, param_grid, cv = 5, n_jobs = -2)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestRegressor(n_estimators=, max_depth=5, min_samples_leaf=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(rforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_rforest = rforest.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y6_pred = rforest.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y6_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y6_pred_v = rforest.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y6_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0: 0.38852359192540425\n",
    "#1: 0.38616747296757176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "actual_vs_pred_plot(y_valid, y6_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Support Vector Regressor MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support vector regressor is another powerful model. It is memory efficient and offers different kernels to choose from. Grid search was used to find the best value of the hyperparameters C, gamma and epsilon. The sigmoid kernel was used along with the default value of epsilon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_basic = SVR(C = 10, gamma = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search to find best value of C, gamma and epsilon and default kernel 'rbf'\n",
    "param_grid  = {'C': [5,7,10,15,20,30],'gamma': [0.001, 0.0001, 0.0011, 0.00011], 'epsilon': [0.1, 0.01, 0.001, 0.005, 0.007, 0.008, 0.009] }\n",
    "\n",
    "# set cross validation to 5\n",
    "clf = GridSearchCV(svr_basic, param_grid, cv = 10, n_jobs = -2)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make final SVR model with best parameters found from grid search\n",
    "svr = make_pipeline(MinMaxScaler(), SVR(C= 5, epsilon= 0.1, gamma=0.0011, kernel = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_svr = svr.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y7_pred = svr.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y7_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y7_pred_v = svr.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y7_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear - 0.4338387095039476\n",
    "# Sigmoid - 0.3900469727418305\n",
    "# With sigmoid as default kernel - 0.39670545624904924\n",
    "# rbf - 0.39420253052849114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_pred_plot(y_valid, y7_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Gradient Boosting Regressor MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient boosting regression is an ensemble of weak prediction models. Two gradient boosting models with different depths were evaluated. The loss was set to ‘huber’ which is a combination of least square regression and a highly robust loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max depth to 4, min_samples_leaf to 15\n",
    "gbr1 = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth = 7,\n",
    "                                min_samples_leaf=7, loss='huber', random_state =42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max depth to 7, min_samples_leaf to 10\n",
    "gbr2 = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth = 9,\n",
    "                                min_samples_leaf=10, loss='huber', random_state =42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(gbr1)\n",
    "cross_validation(gbr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_gbr1 = gbr1.fit(X_train, y_train)\n",
    "model_gbr2 = gbr2.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y_g1_pred = gbr1.predict(X_train)\n",
    "y_g2_pred = gbr2.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y_g1_pred,y_train)\n",
    "rmse(y_g2_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model gbr2 performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y8_pred_v = gbr2.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y8_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for gbr2\n",
    "actual_vs_pred_plot(y_valid, y8_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. STACKED REGRESSOR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final model used is the stacked regressor model. Stacking allows the power of each individual estimator to be used by using their output as a final estimator input. Random forest, Support vector regressor, K -nearest neighbour regressor and ridge regressor were stacked with random forest as the final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Random Forest,Support Vector Regressor and Gradient Boosting to build a stack model because they have lower RMSE comparatively\n",
    "estimators = [('Random Forest', rforest),\n",
    "              (\"Support Vector Regressor\",svr),\n",
    "              (\"K\",neigh),\n",
    "              (\"Ridge\",ridge)\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = StackingRegressor(estimators = estimators, final_estimator = rforest, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "model_stack = stacked.fit(X_train, y_train)\n",
    "\n",
    "#predict value of sale price on the training set\n",
    "y9_pred = stacked.predict(X_train)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y9_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the validation set\n",
    "y9_pred_v = stacked.predict(X_valid)\n",
    "\n",
    "#caculate root mean square error\n",
    "rmse(y9_pred_v, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "actual_vs_pred_plot(y_valid,y9_pred_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear reg                        : 0.42793480397157035\n",
    "- ridge                             : 0.3957886167433282\n",
    "- lasso                             : 0.4059493256188701\n",
    "- k-nearest neighbour(k=5)          : 0.41351487769327555\n",
    "- decision tree(maxdepth=9)         : 0.4583579345988703\n",
    "- random forest                     : 0.38616747296757176\n",
    "- Support Vector Regressor          : 0.3900469727418305\n",
    "- Gradient Boosting Regressor       : 0.4118219430457788\n",
    "- Stacked Regressor model           : 0.3769718491202983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How errors compare:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lowest error is of  : Stacked Regressor model  \n",
    "- The largest error is of : decision tree(maxdepth=9)\n",
    "- Therefore Stacked Regressor model will be applied to the test data as it is the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------------------------- 5. TEST DATA PREDICTION -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"sample_submission.csv\"\n",
    "df_sub = pd.read_csv(csv_path, sep = ',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict value of sale price on the training set\n",
    "y_final_pred = stacked.predict(X_test)\n",
    "\n",
    "y_final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undo the log tranformation to get predictions in terms of original label\n",
    "predictions = np.expm1(y_final_pred)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame()\n",
    "submit['Id'] = test_ID\n",
    "submit['SalePrice'] = predictions\n",
    "submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
